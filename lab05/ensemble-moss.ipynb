{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Red Wine Quality Using Machine Learning  \n",
    "### Nolan Moss\n",
    "\n",
    "In this project, I explore a dataset of red wine physicochemical properties and their associated quality ratings, with the goal of building a machine learning model that can accurately predict wine quality based on chemical attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Imports once at the top, organized\n",
    "# ------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    BaggingClassifier,\n",
    "    VotingClassifier,\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1. Load and Inspect the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1599 entries, 0 to 1598\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   fixed acidity         1599 non-null   float64\n",
      " 1   volatile acidity      1599 non-null   float64\n",
      " 2   citric acid           1599 non-null   float64\n",
      " 3   residual sugar        1599 non-null   float64\n",
      " 4   chlorides             1599 non-null   float64\n",
      " 5   free sulfur dioxide   1599 non-null   float64\n",
      " 6   total sulfur dioxide  1599 non-null   float64\n",
      " 7   density               1599 non-null   float64\n",
      " 8   pH                    1599 non-null   float64\n",
      " 9   sulphates             1599 non-null   float64\n",
      " 10  alcohol               1599 non-null   float64\n",
      " 11  quality               1599 non-null   int64  \n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 150.0 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset (download from UCI and save in the same folder)\n",
    "df = pd.read_csv(\"winequality-red.csv\", sep=\";\")\n",
    "\n",
    "# Display structure and first few rows\n",
    "df.info()\n",
    "df.head()\n",
    "\n",
    "# The dataset includes 11 physicochemical input variables (features):\n",
    "# ---------------------------------------------------------------\n",
    "# - fixed acidity          mostly tartaric acid\n",
    "# - volatile acidity       mostly acetic acid (vinegar)\n",
    "# - citric acid            can add freshness and flavor\n",
    "# - residual sugar         remaining sugar after fermentation\n",
    "# - chlorides              salt content\n",
    "# - free sulfur dioxide    protects wine from microbes\n",
    "# - total sulfur dioxide   sum of free and bound forms\n",
    "# - density                related to sugar content\n",
    "# - pH                     acidity level (lower = more acidic)\n",
    "# - sulphates              antioxidant and microbial stabilizer\n",
    "# - alcohol                % alcohol by volume\n",
    "\n",
    "# The target variable is:\n",
    "# - quality (integer score from 0 to 10, rated by wine tasters)\n",
    "\n",
    "# We will simplify this target into three categories:\n",
    "#   - low (3–4), medium (5–6), high (7–8) to make classification feasible.\n",
    "#   - we will also make this numeric (we want both for clarity)\n",
    "# The dataset contains 1599 samples and 12 columns (11 features + target).\n",
    "\n",
    "\n",
    "\n",
    "# Load spiral dataset\n",
    "##spiral = pd.read_csv(\"spiral.csv\")\n",
    "\n",
    "# Display basic information\n",
    "##spiral.info()\n",
    "\n",
    "# Display first few rows\n",
    "#print(spiral.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2. Prepare the Data\n",
    "Includes cleaning, feature engineering, encoding, splitting, helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function that:\n",
    "\n",
    "# Takes one input, the quality (which we will temporarily name q while in the function)\n",
    "# And returns a string of the quality label (low, medium, high)\n",
    "# This function will be used to create the quality_label column\n",
    "def quality_to_label(q):\n",
    "    if q <= 4:\n",
    "        return \"low\"\n",
    "    elif q <= 6:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"high\"\n",
    "\n",
    "\n",
    "# Call the apply() method on the quality column to create the new quality_label column\n",
    "df[\"quality_label\"] = df[\"quality\"].apply(quality_to_label)\n",
    "\n",
    "\n",
    "# Then, create a numeric column for modeling: 0 = low, 1 = medium, 2 = high\n",
    "def quality_to_number(q):\n",
    "    if q <= 4:\n",
    "        return 0\n",
    "    elif q <= 6:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "\n",
    "df[\"quality_numeric\"] = df[\"quality\"].apply(quality_to_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain what we do and why as you proceed. \n",
    "\n",
    "The data grouped quality scores into categories to simplify analysis and improve interpretability. Then created a numeric version of those categories for use in modeling. This dual labeling one categorical and one numeric helps with both exploratory data analysis and machine learning model development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3. Feature Selection and Justification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input features (X) and target (y)\n",
    "# Features: all columns except 'quality' and 'quality_label' and 'quality_numberic' - drop these from the input array\n",
    "# Target: quality_label (the new column we just created)\n",
    "X = df.drop(columns=[\"quality\", \"quality_label\", \"quality_numeric\"])  # Features\n",
    "y = df[\"quality_numeric\"]  # Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain / introduce your choices.\n",
    "\n",
    "Target-related columns from the feature set are excluded to avoid data leakage. Objective chemical measurements as input features are chosen because these are relevant, independent, and measurable properties of the wine. A numeric target was seleceted to match the classification goal and ensure compatibility with ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4. Split the Data into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split (stratify to preserve class balance)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5.  Evaluate Model Performance (Choose 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a list of  9 model variations. Choose two to focus on for your comparison. \n",
    "\n",
    "Option\tModel Name\tNotes\n",
    "1\tRandom Forest (100)\tA strong baseline model using 100 decision trees.\n",
    "2\tRandom Forest (200, max_depth=10)\tAdds more trees, but limits tree depth to reduce overfitting.\n",
    "3\tAdaBoost (100)\tBoosting method that focuses on correcting previous errors.\n",
    "4\tAdaBoost (200, lr=0.5)\tMore iterations and slower learning for better generalization.\n",
    "5\tGradient Boosting (100)\tBoosting approach using gradient descent.\n",
    "6\tVoting (DT + SVM + NN)\tCombines diverse models by averaging their predictions.\n",
    "7\tVoting (RF + LR + KNN)\tAnother mix of different model types.\n",
    "8\tBagging (DT, 100)\tBuilds many trees in parallel on different samples.\n",
    "9\tMLP Classifier\tA basic neural network with one hidden layer.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to train and evaluate models\n",
    "def evaluate_model(name, model, X_train, y_train, X_test, y_test, results):\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    train_f1 = f1_score(y_train, y_train_pred, average=\"weighted\")\n",
    "    test_f1 = f1_score(y_test, y_test_pred, average=\"weighted\")\n",
    "\n",
    "    print(f\"\\n{name} Results\")\n",
    "    print(\"Confusion Matrix (Test):\")\n",
    "    print(confusion_matrix(y_test, y_test_pred))\n",
    "    print(f\"Train Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Train F1 Score: {train_f1:.4f}, Test F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"Model\": name,\n",
    "            \"Train Accuracy\": train_acc,\n",
    "            \"Test Accuracy\": test_acc,\n",
    "            \"Train F1\": train_f1,\n",
    "            \"Test F1\": test_f1,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to create the different types of ensemble models listed above (you don't need to do all of them yourself. Choose 2 - we have a whole team working on this.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest (100) Results\n",
      "Confusion Matrix (Test):\n",
      "[[  0  13   0]\n",
      " [  0 256   8]\n",
      " [  0  15  28]]\n",
      "Train Accuracy: 1.0000, Test Accuracy: 0.8875\n",
      "Train F1 Score: 1.0000, Test F1 Score: 0.8661\n",
      "\n",
      "MLP Classifier Results\n",
      "Confusion Matrix (Test):\n",
      "[[  0  13   0]\n",
      " [  0 257   7]\n",
      " [  0  30  13]]\n",
      "Train Accuracy: 0.8514, Test Accuracy: 0.8438\n",
      "Train F1 Score: 0.8141, Test F1 Score: 0.8073\n"
     ]
    }
   ],
   "source": [
    "# 1. Random Forest\n",
    "evaluate_model(\n",
    "    \"Random Forest (100)\",\n",
    "    RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    results,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# 9. MLP Classifier \n",
    "evaluate_model(\n",
    "    \"MLP Classifier\",\n",
    "    MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42),\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    results,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 6. Compare Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of All Models:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Train F1</th>\n",
       "      <th>Test F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest (100)</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.88750</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.866056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLP Classifier</td>\n",
       "      <td>0.851446</td>\n",
       "      <td>0.84375</td>\n",
       "      <td>0.814145</td>\n",
       "      <td>0.807318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Train Accuracy  Test Accuracy  Train F1   Test F1\n",
       "0  Random Forest (100)        1.000000        0.88750  1.000000  0.866056\n",
       "1       MLP Classifier        0.851446        0.84375  0.814145  0.807318"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a table of results \n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\nSummary of All Models:\")\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommendation: See if you can add gap calculations to your results and sort the table by test accuracy to find the best models more efficiently. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7. Conclusions and Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RandomForest has excellent performance on the training data, but perfect training accuracy suggests overfitting. It does very well on the test set, especially at correctly identifying Medium quality wines. However, it struggles most with Low-quality wines, probably due to class imbalance.The MLP Classifier is less overfit because tbe performance on train/test is closer together. It has lower overall performance than Random Forest.\n",
    "This is likely impacted by the lack of class balance and possibly not enough hidden complexity for this problem.\n",
    "\n",
    "The Random Forest is more accurate overall but shows signs of overfitting. Still, it performs best in terms of test metrics, so it's likely the better model. MLP Classifier is more balanced and less prone to overfitting, but underperforms compared to Random Forest.\n",
    "\n",
    "Considering another students work with the Voting Classifier, it shows that the ensemble does not overfit as much and is more balanced. It has a slightly lower performance than Random Forest, but it's more conservative and robust."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
